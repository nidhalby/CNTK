{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# CNTK 302: ReasoNet for Machine Comprehension\n",
    "\n",
    "## Introduction and Background\n",
    "\n",
    "In CNTK 204 tutorial, we show how to generate a sequence from an input sequence of data. We have introduced the \"attention mechanism\" which results in superior performance. Research has shown how the human brain pays selective attention to certain parts of the enormous amount of information that comes our way by filtering out extraneous data that is not currently of import for the task at hand. An “attention mechanish” is used as a building block for other machine understanding tasks. \n",
    "\n",
    "This tutorial is an advancement over attention based mechanisms shown earlier. The Reasoning Network [ReasoNet](https://posenhuang.github.io/papers/reasonet_iclr_2017.pdf) has achieved industry-leading results in reading comprehension. The ReasoNet model is a leading model with ExactMatch (EM) and F1 scores (being 73.42 and 81.75, respectively at the time of creation of this tutorial) on the Stanford Question Answering Dataset [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/). \n",
    "\n",
    "**Goal:**\n",
    "\n",
    "The goal of this tutorial is to show how we can teach machines to read, process and comprehend natural language documents. Genuine reading comprehension is extremely challenging as it requires understanding of the documents and performing reasoning involving different context. Enabling a machine to answer a question based on provided passage has gained significant attention enabled by powerful deep learning models.\n",
    "\n",
    "**Problem**\n",
    "\n",
    "In this tutorial we use the [CNN data](https://github.com/deepmind/rc-data) where there are 4 components: (1) query (q), (2) document (d), (3) candidate list (a) and (4) the true answer (A). We train a model that enables machine comprehension with state-of-the-art performance where for a question asked in the context of a paragraph we find the corresponding answer. We will be training our model on triplets as a collection of query, passage and answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data preparation\n",
    "\n",
    "### Download data\n",
    "The data can be downloaded via the follwoing two links: [link1](https://drive.google.com/uc?export=download&id=0BwmD_VLjROrfTTljRDVZMFJnVWM) or [link2](https://github.com/deepmind/rc-data).\n",
    "The downloaded data is packaged as a gz file and needs to be reformatted prior to use in this tutorial. \n",
    "\n",
    "**Note**: This step for the **first** time is going to take **an hour or two** depending on your network and machine configuration since the data involves a large number of small files. In subsequent runs, the locally cached data shall be reused. \n",
    "\n",
    "After unpacking the file, we will generate three folders: training, test, and validation. Each folder contains several file each consisting of a paragraph of text, a question, the corresponding the answer to the questions and a list of entities. Here is an example of one data instance: \n",
    "\n",
    "> __*1:*__  http://web.archive.org/web/20150731215720id_/http://edition.cnn.com/2015/04/07/sport/wladimir-klitschko-ukraine-crisis-boxing/<br/>\n",
    ">__*2:*__  <br/>\n",
    "\n",
    "> **Paragraph**\n",
    "\n",
    ">>__*3:*__  @entity3 ( @entity2 ) @entity1 heavyweight boxing champion @entity0 has an important title defense coming up , but his thoughts continue to be dominated by the ongoing fight for democracy in @entity8 . speaking to @entity2 from his @entity3 training base ahead of the april 25 showdown with @entity12 challenger @entity11 in @entity13 , @entity0 said the crisis in his homeland has left him shocked and upset . \" my country is unfortunately suffering in the war with @entity18 -- not that @entity8 tried to give any aggression to any other nation , in this particular case @entity18 , unfortunately it 's the other way around , \" @entity0 told @entity2 . \" i never thought that our brother folk is going to have war with us , so that @entity8 and @entity18 are going to be divided with blood , \" he added . \" unfortunately , we do n't know how far it 's going to go and how worse it 's going to get . the aggression , in the military presence of ( @entity18 ) soldiers and military equipment in my country , @entity8 , is upsetting . \" @entity0 is the reigning @entity33 , @entity34 , @entity35 and @entity36 champion and has , alongside older brother @entity37 , dominated the heavyweight division in the 21st century . @entity37 , who retired from boxing in 2013 , is a prominent figure in @entity8 politics . the 43 - year - old has led the @entity43 since 2010 and was elected mayor of @entity45 in may last year . tensions in the former @entity48 state remain high despite a ceasefire agreed in february as @entity50 , led by @entity52 chancellor @entity51 and president of france @entity53 , tries to broker a peace deal between the two sides . the crisis in @entity8 began in november 2013 when former president @entity58 scuttled a trade deal with the @entity60 in favor of forging closer economic ties with @entity18 . the move triggered a wave of anti-government protests which came to a head @entity45 's @entity67 in february 2014 when clashes between protesters and government security forces left around 100 dead . the following month , @entity18 troops entered @entity8 's @entity74 peninsula before @entity18 president @entity75 completed the annexation of @entity74 -- a move denounced by most of the world as illegitimate -- after citizens of the region had voted in favor of leaving @entity8 in a referendum . more than 5,000 people have been killed in the conflict to date . \" people are dying in @entity8 every single day , \" @entity0 said . \" i do not want to see it , nobody wants to see it ... it 's hard to believe these days something like that in @entity50 -- and @entity8 is @entity50 -- can happen . \" but with the backing of the international community , @entity0 is confident @entity8 can forge a democratic future rather than slide back towards a @entity48 - era style dictatorship . \" i really wish and want this conflict to be solved and it can only be solved with @entity98 help , \" he said . \" @entity8 is looking forward to becoming a democratic country and live under @entity98 democracy . this is our decision and this is our will to get what we want . \" if somebody wants to try to put ( us ) back to the @entity48 times and be part of the @entity108 , we disagree with that . we want to be in freedom . \" we have achieved many things in moving forward and showed to the world that we do not want to live under a dictatorship . \" @entity0 , whose comments were made as part of a wide - ranging interview for @entity2 's @entity118 series , is routinely kept abreast of developments in @entity8 by brother @entity37 but also returns home whenever he can . \" as much time as i can spend , i am there in the @entity8 . it 's not like i am getting the news from mass media and making my own adjustments and judgments on what 's going on . it 's an actual presence and understanding from the inside ... it obviously affects my life , it affects the life of my family . \" the 39 - year - old and his fiancée @entity137 celebrated happier times last december when the @entity12 actress gave birth to a baby daughter , @entity142 . \" i need to get used to it that i 'm a father , which is really exciting . i hope i 'm going to have a big family with multiple kids , \" he said . @entity0 is n't sure when he 'll finally hang up his gloves . \" i do n't know how long i can last ... motivation and health have to be there to continue . \" but after leaving almost all his boxing opponents battered and bruised -- the @entity8 is seeking an impressive 18th consecutive title defense against @entity11 -- @entity0 is keen to carry on fighting his own and his country 's corner in the opposite way outside the ring . \" i just really want that we 'll have less violence in the world ... i hope in peace we can do anything , but if we have war then it 's definitely going to leave us dull and numb . \" watch @entity0 's @entity118 interview on @entity2 's @entity165 on wednesday april 8 at 1130 , 1245 , 1445 , 2130 , 2245 and 2345 and thursday april 9 at 0445 ( all times gmt ) and here online .<br/>\n",
    ">>__*4:*__  <br/>\n",
    "\n",
    "> **Question**\n",
    ">>__*5:*__  @placeholder faces @entity12 challenger @entity11 in @entity13 on april 25<br/>\n",
    ">>__*6:*__  <br/>\n",
    "\n",
    "> **Answer**\n",
    ">>__*7:*__  @entity0<br/>\n",
    ">>__*8:*__  <br/>\n",
    "\n",
    "> **Entity mapping in pargraph and query**\n",
    ">>__*9:*__  @entity118:Human to Hero<br/>\n",
    ">>__*10:*__  @entity13:New York<br/>\n",
    ">>__*11:*__  @entity137:Hayden Panettiere<br/>\n",
    ">>__*12:*__  @entity12:American<br/>\n",
    ">>__*13:*__  @entity3:Miami<br/>\n",
    ">>__*14:*__  @entity2:CNN<br/>\n",
    ">>__*15:*__  @entity1:World<br/>\n",
    ">>__*16:*__  @entity0:Klitschko<br/>\n",
    ">>__*17:*__  @entity11:Bryant Jennings<br/>\n",
    ">>__*18:*__  @entity8:Ukraine<br/>\n",
    ">>__*19:*__  @entity53:Francois Hollande<br/>\n",
    ">>__*20:*__  @entity52:German<br/>\n",
    ">>__*21:*__  @entity51:Angela Merkel<br/>\n",
    ">>__*22:*__  @entity50:Europe<br/>\n",
    ">>__*23:*__  @entity75:Vladimir Putin<br/>\n",
    ">>__*24:*__  @entity74:Crimea<br/>\n",
    ">>__*25:*__  @entity58:Victor Yanukovych<br/>\n",
    ">>__*26:*__  @entity33:IBF<br/>\n",
    ">>__*27:*__  @entity35:WBO<br/>\n",
    ">>__*28:*__  @entity34:WBA<br/>\n",
    ">>__*29:*__  @entity37:Vitali<br/>\n",
    ">>__*30:*__  @entity36:IBO<br/>\n",
    ">>__*31:*__  @entity18:Russian<br/>\n",
    ">>__*32:*__  @entity98:Western<br/>\n",
    ">>__*33:*__  @entity108:former Soviet Union<br/>\n",
    ">>__*34:*__  @entity142:Kaya<br/>\n",
    ">>__*35:*__  @entity165:World Sport program<br/>\n",
    ">>__*36:*__  @entity45:Kiev<br/>\n",
    ">>__*37:*__  @entity43:Ukrainian Democratic Alliance for Reform<br/>\n",
    ">>__*38:*__  @entity67:Maidan Square<br/>\n",
    ">>__*39:*__  @entity48:Soviet<br/>\n",
    ">>__*40:*__  @entity60:European Union<br/>\n",
    "\n",
    "We will use the following block of code to download and merge each folder of files into a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All necessary data are downloaded to ../Examples/LanguageUnderstanding/ReasoNet/Data\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import sys\n",
    "import tarfile\n",
    "import shutil\n",
    "from io import open\n",
    "# Check for an environment variable defined in CNTK's test infrastructure\n",
    "def is_test(): \n",
    "  return 'CNTK_EXTERNAL_TESTDATA_SOURCE_DIRECTORY' in os.environ\n",
    "  \n",
    "def file_exists(src):\n",
    "  return (os.path.isfile(src) and os.path.exists(src))\n",
    "\n",
    "def download(src, target, cookies=None):\n",
    "  target_dir=os.path.dirname(target)\n",
    "  if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)    \n",
    "  url=src\n",
    "  print(\"Start to download data from {0} to {1}\".format(url, target))\n",
    "  response = requests.get(url, stream=True, cookies=cookies)\n",
    "  with open(target, 'wb') as handle:\n",
    "    for data in response.iter_content(chunk_size=2**20):\n",
    "      if data: handle.write(data)\n",
    "      sys.stdout.write('.')\n",
    "      sys.stdout.flush()\n",
    "  print()\n",
    "  print(\"Finished to download {0} to {1}\".format(url, target))\n",
    "    \n",
    "def merge_files(folder, target):\n",
    "  if os.path.exists(target):\n",
    "    return\n",
    "  count = 0\n",
    "  all_files = os.listdir(folder)\n",
    "  print(\"Start to merge {0} files under folder {1} as {2}\".format(len(all_files), folder, target))\n",
    "    \n",
    "  for f in all_files:\n",
    "    txt=os.path.join(folder, f)\n",
    "    if os.path.isfile(txt):\n",
    "      with open(txt, encoding='utf-8') as sample:\n",
    "        content = sample.readlines()\n",
    "        context = content[2].strip()\n",
    "        query = content[4].strip()\n",
    "        answer = content[6].strip()\n",
    "        entities = []\n",
    "        for k in range(8, len(content)):\n",
    "          entities += [ content[k].strip() ]\n",
    "        with open(target, 'a', encoding='utf-8') as output:\n",
    "          output.write(u\"{0}\\t{1}\\t{2}\\t{3}\\n\".format(query, answer, context, \"\\t\".join(entities)))\n",
    "    count+=1\n",
    "    if count%1000==0:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "  print()\n",
    "  print(\"Finished to merge {0}\".format(target))\n",
    "\n",
    "def download_cnn(target=\".\"):\n",
    "  if os.path.exists(os.path.join(target, \"cnn\")):\n",
    "    print(\"Start to remove dirty folders ...\")\n",
    "    shutil.rmtree(os.path.join(target, \"cnn\"))\n",
    "  if not os.path.exists(target):\n",
    "    os.makedirs(target)\n",
    "  tar_target = os.path.join(target, \"cnn.tar.gz\")\n",
    "  if not file_exists(tar_target):\n",
    "    url=\"https://drive.google.com/uc?export=download&id=0BwmD_VLjROrfTTljRDVZMFJnVWM\"\n",
    "    print(\"Start to download CNN data from {0} to {1}\".format(url, target))    \n",
    "    pre_request = requests.get(url)\n",
    "    confirm_match = re.search(r\"confirm=(.{4})\", pre_request.content.decode(\"utf-8\"))\n",
    "    confirm_url = url + \"&confirm=\" + confirm_match.group(1)\n",
    "    download(confirm_url, tar_target, cookies=pre_request.cookies)\n",
    "    print(\"Finished to download {0} to {1}\".format(url, target))\n",
    "  print(\"Start to uncompress the downloaded file ...\")\n",
    "  tar = tarfile.open(tar_target, mode=\"r:gz\")\n",
    "  tar.extractall(target)  \n",
    "\n",
    "if not is_test():\n",
    "  data_root = \"../Examples/LanguageUnderstanding/ReasoNet/Data\"\n",
    "  raw_train_data=os.path.join(data_root, \"cnn/training.txt\")\n",
    "  raw_test_data=os.path.join(data_root, \"cnn/test.txt\")\n",
    "  raw_validation_data=os.path.join(data_root, \"cnn/validation.txt\")\n",
    "  if not (file_exists(raw_train_data) and file_exists(raw_test_data) and file_exists(raw_validation_data)):\n",
    "    download_cnn(data_root)\n",
    "\n",
    "  merge_files(os.path.join(data_root, \"cnn/questions/training\"), raw_train_data)\n",
    "  merge_files(os.path.join(data_root, \"cnn/questions/test\"), raw_test_data)\n",
    "  merge_files(os.path.join(data_root, \"cnn/questions/validation\"), raw_validation_data)\n",
    "else:\n",
    "  data_root = \"../Examples/LanguageUnderstanding/ReasoNet/Data\"\n",
    "  raw_train_data=os.path.join(data_root, \"cnn_test/training.txt\")\n",
    "  raw_test_data=os.path.join(data_root, \"cnn_test/test.txt\")\n",
    "  raw_validation_data=os.path.join(data_root, \"cnn_test/validation.txt\")\n",
    "print(\"All necessary data are downloaded to {0}\".format(data_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convert to CNTK Text Format\n",
    "\n",
    "In order to take advantage of the scalable readers bundled with CNTK, we need to convert the original data into a column separated format [CNTK text format](https://github.com/Microsoft/CNTK/wiki/BrainScript-CNTKTextFormat-Reader). \n",
    "There are 5 columns/streams in the conveted CTF data file, including context, query, entity indication, label, entity ids. Here is a snippnet of the converted CTF output for the above example input,\n",
    "\n",
    ">  0 |Q 12:1   |C 4:1    |E 1 |L 0 |EID 4:1<br/>\n",
    ">  |Q 1739:1 |C 626:1  |E 0 |L 0 |EID 2:1<br/>\n",
    ">  |Q 14:1   |C 2:1    |E 1 |L 0 |EID 1:1<br/>\n",
    ">  |Q 5453:1 |C 625:1  |E 0 |L 0 |EID 3:1<br/>\n",
    ">  |Q 13:1   |C 1:1    |E 1 |L 0 |EID 9:1<br/>\n",
    ">  |Q 594:1  |C 7562:1 |E 0 |L 0 |EID 2:1<br/>\n",
    ">  |Q 15:1   |C 5284:1 |E 0 |L 0 |EID 4:1<br/>\n",
    ">  |Q 600:1  |C 1245:1 |E 0 |L 0 |EID 14:1<br/>\n",
    ">  |Q 1307:1 |C 3:1    |E 1 |L 1 |EID 13:1<br/>\n",
    ">  |Q 1309:1 |C 616:1  |E 0 |L 0 |EID 15:1<br/>\n",
    ">  |C 620:1  |E 0 |L 0  EID 3:1<br/>\n",
    ">  |C 927:1  |E 0 |L 0 |EID 20:1<br/>\n",
    ">  |C 1115:1 |E 0 |L 0 |EID 9:1<br/>\n",
    ">  |C 1017:1 |E 0 |L 0 |EID 20:1<br/>\n",
    ">  |C 1067:1 |E 0 |L 0 |EID 3:1<br/>\n",
    ">  |C 650:1  |E 0 |L 0 |EID 2:1<br/>\n",
    ">  |C 587:1  |E 0 |L 0 |EID 9:1<br/>\n",
    ">  |C 613:1  |E 0 |L 0 |EID 20:1<br/>\n",
    ">  |C 608:1  |E 0 |L 0 |EID 20:1<br/>\n",
    ">  |C 2892:1 |E 0 |L 0 |EID 9:1<br/>\n",
    ">  |C 1015:1 |E 0 |L 0 |EID 3:1<br/>\n",
    ">  |C 589:1  |E 0 |L 0 |EID 35:1<br/>\n",
    ">  |C 615:1  |E 0 |L 0 |EID 36:1<br/>\n",
    ">  |C 2814:1 |E 0 |L 0 |EID 37:1<br/>\n",
    ">  |C 617:1  |E 0 |L 0 |EID 39:1<br/>\n",
    ">  |C 586:1  |E 0 |L 0 |EID 40:1<br/>\n",
    ">  |C 2090:1 |E 0 |L 0 |EID 40:1<br/>\n",
    ">  |C 1057:1 |E 0 |L 0 |EID 9:1<br/>\n",
    ">  |C 597:1  |E 0 |L 0 |EID 44:1<br/>\n",
    ">  |C 2054:1 |E 0 |L 0 |EID 47:1<br/>\n",
    "\n",
    "The first column is the sequence id, 0. The second is the features of Query delimited by `|Q`, the third is the features of Context delimited by '|C', and the fourth is a boolean to indicate if that word in the Context is an entity, the fifth is the Label which indicate if that word in the context is the answer. The last is the ID of entities in the context.\n",
    "\n",
    "The code below performs the conversion. \n",
    "\n",
    "**Note**: The downloading and conversion can take upto 30 min and requires 11 GB of local disc space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data conversion finished.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "class WordFreq:\n",
    "  def __init__(self, word, id, freq):\n",
    "    self.word = word\n",
    "    self.id = id\n",
    "    self.freq = freq\n",
    "\n",
    "class Vocabulary:\n",
    "  \"\"\"Build word vocabulary with frequency\"\"\"\n",
    "  def __init__(self, name):\n",
    "    self.name = name\n",
    "    self.size = 0\n",
    "    self.__dict = {}\n",
    "    self.__has_index = False\n",
    "    self.__reverse = {}\n",
    "\n",
    "  def push(self, word):\n",
    "    if word in self.__dict:\n",
    "      self.__dict[word].freq += 1\n",
    "    else:\n",
    "      self.__dict[word] = WordFreq(word, len(self.__dict), 1)\n",
    "\n",
    "  def build_index(self, max_size):\n",
    "    def word_cmp(x, y):\n",
    "      if x.freq == y.freq :\n",
    "        return (x.word > y.word) - (x.word < y.word)\n",
    "      else:\n",
    "        return x.freq - y.freq\n",
    "\n",
    "    items = sorted(self.__dict.values(), key=functools.cmp_to_key(word_cmp), reverse=True)\n",
    "    if len(items)>max_size:\n",
    "      del items[max_size:]\n",
    "    self.size=len(items)\n",
    "    self.__dict.clear()\n",
    "    for it in items:\n",
    "      it.id = len(self.__dict)\n",
    "      self.__dict[it.word] = it\n",
    "    self.__has_index = True\n",
    "\n",
    "  def save(self, dst):\n",
    "    if not self.__has_index:\n",
    "      self.build_index(sys.maxsize)\n",
    "    if self.name != None:\n",
    "      dst.write(u\"{0}\\t{1}\\n\".format(self.name, self.size))\n",
    "    for it in sorted(self.__dict.values(), key=lambda it:it.id):\n",
    "      dst.write(u\"{0}\\t{1}\\t{2}\\n\".format(it.word, it.id, it.freq))\n",
    "\n",
    "  def load(self, src):\n",
    "    line = src.readline()\n",
    "    if line == \"\":\n",
    "      return\n",
    "    line = line.rstrip('\\n')\n",
    "    head = line.split()\n",
    "    max_size = sys.maxsize\n",
    "    if len(head) == 2:\n",
    "      self.name = head[0]\n",
    "      max_size = int(head[1])\n",
    "    cnt = 0\n",
    "    while cnt < max_size:\n",
    "      line = src.readline()\n",
    "      if line == \"\":\n",
    "        break\n",
    "      line = line.rstrip('\\n')\n",
    "      items = line.split()\n",
    "      self.__dict[items[0]] = WordFreq(items[0], int(items[1]), int(items[2]))\n",
    "      cnt += 1\n",
    "    self.size = len(self.__dict)\n",
    "    self.__has_index = True\n",
    "\n",
    "  def __getitem__(self, key):\n",
    "    if key in self.__dict:\n",
    "      return self.__dict[key]\n",
    "    else:\n",
    "      return None\n",
    "\n",
    "  def values(self):\n",
    "    return self.__dict.values()\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.size\n",
    "\n",
    "  def __contains__(self, q):\n",
    "    return q in self.__dict\n",
    "\n",
    "  def lookup_by_id(self, id):\n",
    "    if self.__reverse is None or len(self.__reverse) == 0:\n",
    "      self.__reverse = {}\n",
    "      for item in self.__dict.items():\n",
    "        self.__reverse[item[1].id]=item[0]\n",
    "    return self.__reverse[id]\n",
    "\n",
    "  @staticmethod\n",
    "  def is_cnn_entity(word):\n",
    "    return word.startswith('@entity') or word.startswith('@placeholder')\n",
    "\n",
    "  @staticmethod\n",
    "  def load_vocab(vocab_src):\n",
    "    \"\"\"\n",
    "    Loa vocabulary from file.\n",
    "\n",
    "    Args:\n",
    "      vocab_src (`str`): the file stored with the vocabulary data\n",
    "      \n",
    "    Returns:\n",
    "      :class:`Vocabulary`: Vocabulary of the entities\n",
    "      :class:`Vocabulary`: Vocabulary of the words\n",
    "    \"\"\"\n",
    "    word_vocab = Vocabulary(\"WordVocab\")\n",
    "    entity_vocab = Vocabulary(\"EntityVocab\")\n",
    "    with open(vocab_src, 'r', encoding='utf-8') as src:\n",
    "      entity_vocab.load(src)\n",
    "      word_vocab.load(src)\n",
    "    return entity_vocab, word_vocab\n",
    "\n",
    "  @staticmethod\n",
    "  def build_vocab(input_src, vocab_dst, max_size=50000):\n",
    "    \"\"\"\n",
    "    Build vocabulary from raw corpus file.\n",
    "\n",
    "    Args:\n",
    "      input_src (`str`): the path of the corpus file\n",
    "      vocab_dst (`str`): the path of the vocabulary file to save the built vocabulary\n",
    "      max_size (`int`): the maxium size of the word vocabulary\n",
    "    Returns:\n",
    "      :class:`Vocabulary`: Vocabulary of the entities\n",
    "      :class:`Vocabulary`: Vocabulary of the words\n",
    "    \"\"\"\n",
    "    # Leave the first as Unknown\n",
    "    max_size -= 1\n",
    "    word_vocab = Vocabulary(\"WordVocab\")\n",
    "    entity_vocab = Vocabulary(\"EntityVocab\")\n",
    "    linenum = 0\n",
    "    print(\"Start build vocabulary from {0} with maxium words {1}. Saved to {2}\"\\\n",
    "          .format(input_src, max_size, vocab_dst))\n",
    "    with open(input_src, 'r', encoding='utf-8') as src:\n",
    "      all_lines = src.readlines()\n",
    "      print(\"Total lines to process: {0}\".format(len(all_lines)))\n",
    "      for line in all_lines:\n",
    "        line = line.strip('\\n')\n",
    "        ans, query_words, context_words = Vocabulary.parse_corpus_line(line)\n",
    "        \n",
    "        # Process all query words\n",
    "        for q in query_words:\n",
    "          if Vocabulary.is_cnn_entity(q):\n",
    "          #if q.startswith('@'):\n",
    "            entity_vocab.push(q)\n",
    "          else:\n",
    "            word_vocab.push(q)\n",
    "         \n",
    "        # Process all context words\n",
    "        for q in context_words:\n",
    "          #if q.startswith('@'):\n",
    "          if Vocabulary.is_cnn_entity(q):\n",
    "            entity_vocab.push(q)\n",
    "          else:\n",
    "            word_vocab.push(q)\n",
    "        linenum += 1\n",
    "        if linenum%1000==0:\n",
    "          sys.stdout.write(\".\")\n",
    "          sys.stdout.flush()\n",
    "    print()\n",
    "    entity_vocab.build_index(max_size)\n",
    "    word_vocab.build_index(max_size)\n",
    "    with open(vocab_dst, 'w', encoding='utf-8') as dst:\n",
    "      entity_vocab.save(dst)\n",
    "      word_vocab.save(dst)\n",
    "    print(\"Finished to generate vocabulary from: {0}\".format(input_src))\n",
    "    return entity_vocab, word_vocab\n",
    "\n",
    "  @staticmethod\n",
    "  def parse_corpus_line(line):\n",
    "    \"\"\"\n",
    "    Parse bing corpus line to answer, query and context.\n",
    "\n",
    "    Args:\n",
    "      line (`str`): A line of text of bing corpus\n",
    "    Returns:\n",
    "      :`str`: Answer word\n",
    "      :`str[]`: Array of query words\n",
    "      :`str[]`: Array of context/passage words\n",
    "\n",
    "    \"\"\"\n",
    "    data = line.split('\\t')\n",
    "    query = data[0]\n",
    "    answer = data[1]\n",
    "    context = data[2]\n",
    "    query_words = query.split()\n",
    "    context_words = context.split()\n",
    "    return answer, query_words, context_words\n",
    "\n",
    "  def build_corpus(entities, words, corpus, output, max_seq_len=100000):\n",
    "    \"\"\"\n",
    "    Build featurized corpus and store it in CNTK Text Format.\n",
    "\n",
    "    Args:\n",
    "      entities (class:`Vocabulary`): The entities vocabulary\n",
    "      words (class:`Vocabulary`): The words vocabulary\n",
    "      corpus (`str`): The file path of the raw corpus\n",
    "      output (`str`): The file path to store the featurized corpus data file\n",
    "    \"\"\"\n",
    "    seq_id = 0\n",
    "    print(\"Start to build CTF data from: {0}\".format(corpus))\n",
    "    with open(corpus, 'r', encoding = 'utf-8') as corp:\n",
    "      with open(output, 'w', encoding = 'utf-8') as outf:\n",
    "        all_lines = corp.readlines()\n",
    "        print(\"Total lines to prcess: {0}\".format(len(all_lines)))\n",
    "        \n",
    "        # Iterate through all the lines in the corpus\n",
    "        for line in all_lines:\n",
    "          line = line.strip('\\n')\n",
    "          ans, query_words, context_words = Vocabulary.parse_corpus_line(line)\n",
    "          ans_item = entities[ans]\n",
    "          query_ids = []\n",
    "          context_ids = []\n",
    "          is_entity = []\n",
    "          entity_ids = []\n",
    "          labels = []\n",
    "          pos = 0\n",
    "          answer_idx = None\n",
    "            \n",
    "          # Process context words  \n",
    "          for q in context_words:\n",
    "            if Vocabulary.is_cnn_entity(q):\n",
    "              item = entities[q]\n",
    "              context_ids += [ item.id + 1 ]\n",
    "              entity_ids += [ item.id + 1 ]\n",
    "              is_entity += [1]\n",
    "              if ans_item.id == item.id:\n",
    "                labels += [1]\n",
    "                answer_idx = pos\n",
    "              else:\n",
    "                labels += [0]\n",
    "            else:\n",
    "              item = words[q]\n",
    "              context_ids += [ (item.id + 1 + entities.size) if item != None else 0 ]\n",
    "              is_entity += [0]\n",
    "              labels += [0]\n",
    "            pos += 1\n",
    "            if (pos >= max_seq_len):\n",
    "              break\n",
    "          if answer_idx is None:\n",
    "            continue\n",
    "          \n",
    "          # Process query words\n",
    "          for q in query_words:\n",
    "            if Vocabulary.is_cnn_entity(q):\n",
    "              item = entities[q]\n",
    "              query_ids += [ item.id + 1 ]\n",
    "            else:\n",
    "              item = words[q]\n",
    "              query_ids += [ (item.id + 1 + entities.size) if item != None else 0 ]\n",
    "                \n",
    "          #Write featurized ids\n",
    "          outf.write(u\"{0}\".format(seq_id))\n",
    "          for i in range(max(len(context_ids), len(query_ids))):\n",
    "            if i < len(query_ids):\n",
    "              outf.write(u\" |Q {0}:1\".format(query_ids[i]))\n",
    "            if i < len(context_ids):\n",
    "              outf.write(u\" |C {0}:1\".format(context_ids[i]))\n",
    "              outf.write(u\" |E {0}\".format(is_entity[i]))\n",
    "              outf.write(u\" |L {0}\".format(labels[i]))\n",
    "            if i < len(entity_ids):\n",
    "              outf.write(u\" |EID {0}:1\".format(entity_ids[i]))\n",
    "            outf.write(u\"\\n\")\n",
    "          seq_id += 1\n",
    "          if seq_id%1000 == 0:\n",
    "            sys.stdout.write(\".\")\n",
    "            sys.stdout.flush()\n",
    "    print()\n",
    "    print(\"Finished to build corpus from {0}\".format(corpus))\n",
    "if not is_test():  \n",
    "  vocab_path=os.path.join(data_root, \"cnn/cnn.vocab\")\n",
    "  train_ctf=os.path.join(data_root, \"cnn/training.ctf\")\n",
    "  test_ctf=os.path.join(data_root, \"cnn/test.ctf\")\n",
    "  validation_ctf=os.path.join(data_root, \"cnn/validation.ctf\")\n",
    "  vocab_size=101000\n",
    "  test_size = 2291183\n",
    "  train_size = 289716292\n",
    "  validation_size = 2993016\n",
    "else:\n",
    "  vocab_path=os.path.join(data_root, \"cnn_test/cnn.vocab\")\n",
    "  train_ctf=os.path.join(data_root, \"cnn_test/training.ctf\")\n",
    "  test_ctf=os.path.join(data_root, \"cnn_test/test.ctf\")\n",
    "  validation_ctf=os.path.join(data_root, \"cnn_test/validation.ctf\")\n",
    "  test_size = 379913\n",
    "  train_size = 1539195\n",
    "  validation_size = 392389\n",
    "    \n",
    "vocab_size=101000\n",
    "if not (file_exists(train_ctf) and file_exists(test_ctf) and file_exists(validation_ctf)):\n",
    "  entity_vocab, word_vocab = Vocabulary.build_vocab(raw_train_data, vocab_path, vocab_size)\n",
    "  Vocabulary.build_corpus(entity_vocab, word_vocab, raw_train_data, train_ctf)\n",
    "  Vocabulary.build_corpus(entity_vocab, word_vocab, raw_test_data, test_ctf)\n",
    "  Vocabulary.build_corpus(entity_vocab, word_vocab, raw_validation_data, validation_ctf)\n",
    "print(\"Training data conversion finished.\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Get GloVe pre-trained embedding data\n",
    "In the paper, they used GloVe pre-trained embedding data for the embedding initialization. The code below is used to download it from the web site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe embedding data downloaded.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "def download_glove_retrained_embedding(target=\".\"):\n",
    "  url=\"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "  if os.path.exists(os.path.join(target, \"glove\")):\n",
    "    shutil.rmtree(os.path.join(target, \"glove\"))\n",
    "  zip_target = os.path.join(target, \"glove.6B.zip\") \n",
    "  target=os.path.join(target, \"glove\")\n",
    "  if not os.path.exists(target):\n",
    "    os.makedirs(target)\n",
    "  print(\"Start to download GloVe pretrained embedding data from {0} to {1}\".format(url, target))\n",
    "  \n",
    "  if not file_exists(zip_target):\n",
    "    download(url, zip_target)\n",
    "    \n",
    "  print(\"Start to unzip the downloaded file ...\")\n",
    "  zipf = zipfile.ZipFile(zip_target, mode='r')\n",
    "  zipf.extractall(target)\n",
    "  print(\"Finished to download {0} to {1}\".format(url, target))\n",
    "\n",
    "if not file_exists(os.path.join(data_root, \"glove/glove.6B.300d.txt\")):\n",
    "  download_glove_retrained_embedding(data_root)\n",
    "\n",
    "print(\"GloVe embedding data downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Basic CNTK imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import cntk\n",
    "from cntk import Trainer, Axis, device, combine\n",
    "from cntk.layers.blocks import Stabilizer, _initializer_for,  _INFERRED, Parameter, GRU\n",
    "from cntk.layers import Recurrence, Dense\n",
    "from cntk.ops import sequence, reduce_sum, \\\n",
    "    parameter, times, element_times, plus, placeholder, reshape, constant, sigmoid, \\\n",
    "    times_transpose, greater, element_divide, element_select, exp, input\n",
    "from cntk.losses import cosine_distance\n",
    "from cntk.internal import _as_tuple, sanitize_input\n",
    "from cntk.initializer import uniform, glorot_uniform\n",
    "from cntk.io import MinibatchSource, CTFDeserializer, StreamDef, StreamDefs\n",
    "import cntk.ops as ops\n",
    "import cntk.learners as learners\n",
    "\n",
    "# Select the right target device when this notebook is being tested\n",
    "# Currently supported only for GPU \n",
    "\n",
    "if 'TEST_DEVICE' in os.environ:\n",
    "    if os.environ['TEST_DEVICE'] == 'cpu':\n",
    "        raise ValueError('This notebook is currently not support on CPU') \n",
    "    else:\n",
    "        cntk.device.set_default_device(cntk.device.gpu(0))\n",
    "cntk.device.set_default_device(cntk.device.gpu(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Utils\n",
    "Some utils will used during model creation and training stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Logger\n",
    "We use logger to write information both to console and a file on disk, so that we can check the inforamtion after the process exits. CNTK has prepackaged loggers as ProgressPrinter and integration with TensorBoard for visualization. However, in this case we share boiler plate code for folks to write custom logger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "class logger:\n",
    "  __name=''\n",
    "  __logfile=''\n",
    "\n",
    "  @staticmethod\n",
    "  def init(name=''):\n",
    "    if not os.path.exists(\"model\"):\n",
    "      os.mkdir(\"model\")\n",
    "    if not os.path.exists(\"log\"):\n",
    "      os.mkdir(\"log\")\n",
    "    if name=='' or name is None:\n",
    "      logger.__name='train'\n",
    "    else:\n",
    "      logger.__name=name\n",
    "    logger.__logfile = 'log/{}_{}.log'.format(logger.__name, datetime.now().strftime(\"%m-%d_%H.%M.%S\"))\n",
    "    if os.path.exists(logger.__logfile):\n",
    "      os.remove(logger.__logfile)\n",
    "    print('Log with log file: {0}'.format(logger.__logfile))\n",
    "\n",
    "  @staticmethod\n",
    "  def log(message, toconsole=True):\n",
    "    if logger.__logfile == '' or logger.__logfile is None:\n",
    "      logger.init()\n",
    "    if toconsole:\n",
    "      print(message)\n",
    "    with open(logger.__logfile, 'a', encoding='utf-8') as logf:\n",
    "      logf.write(u\"{}| {}\\n\".format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), message))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ReasoNet Model\n",
    "\n",
    "Single-turn reasoning models use attention mechanisms with associated deep learning models to emphasize speciﬁc parts of the document which are relevant to the query. However, for many sophisticated comprehension tasks, a human reader often revisits some speciﬁc passage or the question to gain a better understanding of the content. Recent work uses multiple turns to infer the relation between query, document, and answer. This approach has been demonstrated to produce superior results. We summarize the essence of the [original paper](https://posenhuang.github.io/papers/reasonet_iclr_2017.pdf) by Shen et. al. in the remainder of this section.\n",
    "\n",
    "Existing multi-turn models have a ﬁxed number of hops or iterations in their inference, i.e., with predetermined reasoning depth, without regard to the complexity of each individual query or document. However, a human reader may read a document several times and stop when the question in mind has been adequately understood (reaching a certain level of confidence) or terminate after a certain number of tries. \n",
    "\n",
    "ReasoNet tries to mimic the inference process of human readers. With a question in mind, ReasoNet reads a document repeatedly, each time focusing on different parts of the document until a satisfactory answer is found or formed. Moreover, unlike previous approaches using ﬁxed numbers of hops or iterations, ReasoNet introduces a termination state in the inference. This state can decide whether to continue the inference to the next turn after digesting intermediate information, or to terminate the whole inference when it concludes that existing information is sufﬁcient to yield an answer. The number of turns is dynamically modeled by both the document and the query, and will be learned automatically according to the difﬁculty of the problem.\n",
    "\n",
    "![](http://cntk.ai/jup/CNTK_302_reasonet.png)\n",
    "\n",
    "The figure above shows the ResonNet architecture. The model ius made of several components:\n",
    "\n",
    "### VocabSize: \n",
    "\n",
    "For training our ReasoNet, we keep the most frequent|V| = 101k words (not including 584 entities and 1 placeholder marker) in the CNN dataset. \n",
    "\n",
    "### Embedding Layer: \n",
    "\n",
    "We choose word embedding size d = 300, and use the 300 dimensional pretrained GloVe word embeddings (Pennington et al., 2014) for initialization. We also apply dropout with probability 0.2 to the embedding layer. \n",
    "\n",
    "In this implementation we apply a special policy to train the embedding layer. For entities in the context/paragraph, we just use fixed random vectors as the embedding (we do not update them during training). For other words in the context/paragraph and query, we will initialize the embedding using glorot uniform initialization or loading from an existing embedding matrix, e.g. GloVe embedding. The later embedding is updated during training stage. \n",
    "\n",
    "We need to customize existing initializer or embedding lookup funciton in CNTK. Thus, we impliment  `create_random_matrix` and `load_embedding` to create *random initialization matrix* and *load existing embedding matrix*. The class `uniform_initializer` will be used by `load_embedding` to initialize *enities* and other words that cannot be found in the existing embedding matrix (looking up table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class uniform_initializer:\n",
    "  def __init__(self, scale=1, bias=0, seed=0):\n",
    "    self.seed = seed\n",
    "    self.scale = scale\n",
    "    self.bias = bias\n",
    "    np.random.seed(self.seed)\n",
    "\n",
    "  def reset(self):\n",
    "    np.random.seed(self.seed)\n",
    "\n",
    "  def next(self, size=None):\n",
    "    return np.random.uniform(0, 1, size)*self.scale + self.bias\n",
    "\n",
    "def create_random_matrix(rows, columns):\n",
    "  scale = math.sqrt(6/(rows+columns))*2\n",
    "  rand = uniform_initializer(scale, -scale/2)\n",
    "  embedding = [None]*rows\n",
    "  for i in range(rows):\n",
    "    embedding[i] = np.array(rand.next(columns), dtype=np.float32)\n",
    "  return np.ndarray((rows, columns), dtype=np.float32, buffer=np.array(embedding))\n",
    "\n",
    "def load_embedding(embedding_path, vocab_path, dim, init=None):\n",
    "  entity_vocab, word_vocab = Vocabulary.load_vocab(vocab_path)\n",
    "  vocab_dim = len(entity_vocab) + len(word_vocab) + 1\n",
    "  entity_size = len(entity_vocab)\n",
    "  item_embedding = [None]*vocab_dim\n",
    "  with open(embedding_path, 'r', encoding='utf-8') as embedding:\n",
    "    for line in embedding.readlines():\n",
    "      line = line.strip('\\n')\n",
    "      item = line.split(' ')\n",
    "      if item[0] in word_vocab:\n",
    "        item_embedding[word_vocab[item[0]].id + entity_size + 1] = \\\n",
    "        np.array(item[1:], dtype=\"|S\").astype(np.float32)\n",
    "  if init != None:\n",
    "    init.reset()\n",
    "\n",
    "  for i in range(vocab_dim):\n",
    "    if item_embedding[i] is None:\n",
    "      if init:\n",
    "        item_embedding[i] = np.array(init.next(dim), dtype=np.float32)\n",
    "      else:\n",
    "        item_embedding[i] = np.array([0]*dim, dtype=np.float32)\n",
    "  return np.ndarray((vocab_dim, dim), dtype=np.float32, buffer=np.array(item_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Bi-GRUEncoder: \n",
    "\n",
    "We apply bi-directional GRU for encoding query and passage into vector representations. We set the number of hidden units to be 256 for the CNN dataset. The recurrent weights of GRUs are initialized with random orthogonal matrices. The other weights in GRU cell are initialized from a uniform distribution between −0.01 and 0.01. We use a shared GRU model for both query and passage. We use the [`GRU`](https://cntk.ai/pythondocs/cntk.layers.html?highlight=gru#cntk.layers.blocks.GRU) block in the Layers library.\n",
    "\n",
    "#### InternalStateController: \n",
    "\n",
    "We choose GRU model as the internal state controller. The number of hidden units in the GRU state controller is 256 for CNN. The initial state of the GRU controller is set to be the last-word of the query representation by a bidirectional-GRU encoder. \n",
    "\n",
    "### Memory and Attention: \n",
    "\n",
    "The memory of the ReasoNet on CNN dataset is composed of query memory and passage memory. $M = (M^{query},M^{doc})$, where $M^{query}$ and $M^{doc}$ are extracted from query bidirectional-GRU encoder and passage bidirectional-GRU encoder respectively. We choose projected cosine similarity function as the attention module. \n",
    "\n",
    "#### Projected Cosine Similarity\n",
    "\n",
    "The *projected cosine similarity* between *internal contoral status* at time step $t$ and  the $i^{th}$ word in the *document* is computed as,\n",
    "\n",
    "$$\n",
    "sim_{t,i}^{doc}= cos\\left(w_1^{doc}m_i^{doc}, w_2^{doc}s_t\\right)\n",
    "$$\n",
    "\n",
    "where $w_1^{doc}$ and $w_2^{doc}$ are project matrices of demension $hidden\\_dim * attention\\_dim$, $m_i^{doc}$ is  the memory vector of the $i^{th}$ word in the document and $s_t$ is the *internal control status* at time step $t$.\n",
    "\n",
    "Similar to the document, the *projected cosine similarity* between *internal contoral status* at time step $t$ and the $i^{th}$ word in the *query* is computed as,\n",
    "\n",
    "$$\n",
    "sim_{t,i}^{query}= cos\\left(w_1^{query}m_i^{query}, w_2^{query}s_t\\right)\n",
    "$$\n",
    "\n",
    "where $w_1^{query}$ and $w_2^{query}$ are project matrices of demension $hidden\\_dim * attention\\_dim$, $m_i^{doc}$ is  the memory vector of the $i^{th}$ word in the query and $s_t$ is the *internal control status* at time step $t$.\n",
    "The *query attention* can be derived similarly.\n",
    "\n",
    "\n",
    "### Attention Score\n",
    "The *attention score* between *internal contoral status* at time step $t$ and the $i^{th}$ word in the *document* is computed as,\n",
    "$$\n",
    "a_{t,i}^{doc}=softmax_{i=1,...,\\left|M^{doc}\\right|}{\\gamma sim_{t,i}^{doc}}\n",
    "$$\n",
    "where, $\\gamma$ is a constant which is set as 10 in the paper to improve numeric stability.\n",
    "\n",
    "The *attention score* between *internal contoral status* at time step $t$ and the $i^{th}$ word in the *query* is computed as,\n",
    "$$\n",
    "a_{t,i}^{query}=softmax_{i=1,...,\\left|M^{query}\\right|}{\\gamma sim_{t,i}^{query}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def project_cosine(project_dim, init = glorot_uniform(), name=''):\n",
    "  \"\"\"\n",
    "  Compute the project cosine similarity of two input sequences, \n",
    "  where each of the input will be projected to a new dimention space (project_dim) via Wi/Wm\n",
    "  \"\"\"\n",
    "  Wi = Parameter(_INFERRED + (project_dim,), init = init, name='Wi')\n",
    "  Wm = Parameter(_INFERRED + (project_dim,), init = init, name='Wm')\n",
    "\n",
    "  status = placeholder(name='status')\n",
    "  memory = placeholder(name='memory')\n",
    "\n",
    "  projected_status = times(status, Wi, name = 'projected_status')   \n",
    "  projected_memory = times(memory, Wm, name = 'projected_memory')\n",
    "  status_br = sequence.broadcast_as(projected_status, projected_memory, name='status_broadcast')\n",
    "  sim = cosine_distance(status_br, projected_memory, name= name)\n",
    "  return sim\n",
    "\n",
    "def attention_score(att_dim, init = glorot_uniform(), name=''):\n",
    "  \"\"\"\n",
    "  Compute the attention score, \n",
    "  where each of the input will be projected to a new dimention space (att_dim) via Wi/Wm\n",
    "  \"\"\"\n",
    "  sim = project_cosine(att_dim, init, name= name+ '_sim')\n",
    "  return sequence.softmax(sim, name = name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Temination gate\n",
    "\n",
    "This gate determines whether to stop further reasoning and execute the Answer module at $t$ time step. Otherwise the ReasoNet will generate an attention vector for the next time step and update the next internal state. We adopt a logistical regression to model the termination variable at each time step and compute the termination probability in each time step is as,\n",
    "$$\n",
    "f_t\\left(s_t;\\theta_t\\right)=sigmoid\\left(w_ts_t+b_t\\right), where\\ \\theta_t=\\left(w_t, b_t\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def termination_gate(init = glorot_uniform(), name=''):\n",
    "  return Dense(1, activation = sigmoid, init=init, name= name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Answer module\n",
    "\n",
    "When the terminate gate output is set to true, the answer module activates. We first compute the *answer attention score* between *internal control status* and each word in the *document*. We treat it as the *probability/score* of each word to be the correct answer. Then sum up all the *probability/score* of the same *entity* in the *document* as the *probability* of the *entity* to be the correct answer at time step $t$ which is the same as ASR. The final answer is an average of all the time steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Reader\n",
    "\n",
    "The data is stored in CNTK Text Format and we need to create a reader to consume the data. There are 5 columns/streams in the data file, e.g. *context*, *query*, *entity indication*, *label*, *entity ids*. And we use `bind_data` function to bind the *streams* with CNTK functions' (e.g., *model*, *loss*) *arguments* based on their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_reader(path, vocab_dim, entity_dim, randomize):\n",
    "  \"\"\"\n",
    "  Create data reader for the model\n",
    "  Args:\n",
    "    path: The data path\n",
    "    vocab_dim: The dimention of the vocabulary\n",
    "    entity_dim: The dimention of entities\n",
    "    randomize: Where to shuffle the data before feed into the trainer\n",
    "  \"\"\"\n",
    "  return MinibatchSource(CTFDeserializer(path, StreamDefs(\n",
    "    context  = StreamDef(field='C', shape=vocab_dim, is_sparse=True),\n",
    "    query    = StreamDef(field='Q', shape=vocab_dim, is_sparse=True),\n",
    "    entities  = StreamDef(field='E', shape=1, is_sparse=False),\n",
    "    label   = StreamDef(field='L', shape=1, is_sparse=False),\n",
    "    entity_ids   = StreamDef(field='EID', shape=entity_dim, is_sparse=True)\n",
    "    )), randomize=randomize)\n",
    "\n",
    "def bind_data(func, data):\n",
    "  \"\"\"\n",
    "  Bind data outputs to cntk function arguments based on the argument name\n",
    "  \"\"\"\n",
    "  bind = {}\n",
    "  for arg in func.arguments:\n",
    "    if arg.name == 'query':\n",
    "      bind[arg] = data.streams.query\n",
    "    if arg.name == 'context':\n",
    "      bind[arg] = data.streams.context\n",
    "    if arg.name == 'entity_ids_mask':\n",
    "      bind[arg] = data.streams.entities\n",
    "    if arg.name == 'labels':\n",
    "      bind[arg] = data.streams.label\n",
    "    if arg.name == 'entity_ids':\n",
    "      bind[arg] = data.streams.entity_ids\n",
    "  return bind\n",
    "\n",
    "def get_context_bind_stream(bind):\n",
    "  for key in bind.keys():\n",
    "    if key.name == 'context':\n",
    "      return key\n",
    "  return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Model\n",
    "\n",
    "#### Model parameters\n",
    "We use `model_params` to wrapper the parameters to create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class model_params:\n",
    "  def __init__(self, vocab_dim, entity_dim, hidden_dim, embedding_dim=100, embedding_init=None, \n",
    "               share_rnn_param=False, max_rl_steps=5, dropout_rate=None, attention_dim=384, \n",
    "               init=glorot_uniform(), model_name='rsn'):\n",
    "    self.vocab_dim = vocab_dim\n",
    "    self.entity_dim = entity_dim\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.embedding_init = embedding_init\n",
    "    self.max_rl_steps = max_rl_steps\n",
    "    self.dropout_rate = dropout_rate\n",
    "    self.init = init\n",
    "    self.model_name = model_name\n",
    "    self.share_rnn_param = share_rnn_param\n",
    "    self.attention_dim = attention_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### ReasoNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def reasonet_model(context_memory, query_memory, init_status, hidden_dim, att_dim, \n",
    "                    max_steps = 5, init = glorot_uniform()):\n",
    "  \"\"\"\n",
    "  Create the attention model for reasonet\n",
    "  Args:\n",
    "    context_memory: Context memory\n",
    "    query_memory: Query memory\n",
    "    init_status: Intialize status\n",
    "    hidden_dim: The dimention of hidden state\n",
    "    att_dim: The dimention of attention\n",
    "    max_step: Maxuim number of step to revisit the context memory\n",
    "  \"\"\"\n",
    "\n",
    "  #Instantiate the GRU cell\n",
    "  gru = GRU((hidden_dim*2, ), name='control_status')\n",
    "\n",
    "  status = init_status\n",
    "  output = [None]*max_steps*2\n",
    "  sum_prob = None\n",
    "    \n",
    "  # Compute the context, query and candidate attention scores  \n",
    "  context_attention_score = attention_score(att_dim, name='context_attention')\n",
    "  query_attention_score = attention_score(att_dim, name='query_attention')\n",
    "  answer_attention_score = attention_score(att_dim, name='candidate_attention')\n",
    "    \n",
    "  # Termination gate to compute if one should continue or stop \n",
    "  stop_gate = termination_gate(name='terminate_prob')\n",
    "  prev_stop = 0\n",
    "\n",
    "  # Iterate for maximum number of steps\n",
    "  for step in range(max_steps):\n",
    "    \n",
    "    # Compute the context and query attention weight\n",
    "    context_attention_weight = context_attention_score(status, context_memory)\n",
    "    query_attention_weight = query_attention_score(status, query_memory)\n",
    "    \n",
    "    # Compute the aggretated contribution from context and query; and splice combined attentions\n",
    "    context_attention = sequence.reduce_sum(times(context_attention_weight, context_memory), name='C-Att')\n",
    "    query_attention = sequence.reduce_sum(times(query_attention_weight, query_memory), name='Q-Att')\n",
    "    attention = ops.splice(query_attention, context_attention, name='att-sp')\n",
    "    \n",
    "    # Read the output of the GRU cell\n",
    "    status = gru(status, attention).output\n",
    "    \n",
    "    # Based on the status and the context determine the answer attention\n",
    "    ans_attention = answer_attention_score(status, context_memory)\n",
    "    output[step*2] = ans_attention\n",
    "    \n",
    "    # Based on the output of the GRU cell, evaluate the termination probability\n",
    "    termination_prob = stop_gate(status)\n",
    "\n",
    "    # Compute the probability to stop comprehension\n",
    "    if step < max_steps -1:\n",
    "      stop_prob = prev_stop + ops.log(termination_prob, name='log_stop')\n",
    "    else:\n",
    "      stop_prob = prev_stop\n",
    "    output[step*2+1] = sequence.broadcast_as(ops.exp(stop_prob, name='exp_log_stop'), \n",
    "                                             output[step*2], name='Stop_{0}'.format(step))\n",
    "    prev_stop += ops.log(1-termination_prob, name='log_non_stop')\n",
    "\n",
    "  final_ans = None\n",
    "  for step in range(max_steps):\n",
    "    if final_ans is None:\n",
    "      final_ans = output[step*2] * output[step*2+1]\n",
    "    else:\n",
    "      final_ans += output[step*2] * output[step*2+1]\n",
    "  results = combine(output + [ final_ans ], name='Attention_func')\n",
    "  return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Define  the network\n",
    "#### Dynamic axes in CNTK (Key concept)\n",
    "One of the important concepts in understanding CNTK is the idea of two types of axes:\n",
    "* static axes, which are the traditional axes of a variable's shape, and\n",
    "* dynamic axes, which have dimensions that are unknown until the variable is bound to real data at computation time.\n",
    "\n",
    "The dynamic axes are particularly important in the world of recurrent neural networks. Instead of having to decide a maximum sequence length ahead of time, padding your sequences to that size, and wasting computation, CNTK's dynamic axes allow for variable sequence lengths that are automatically packed in minibatches to be as efficient as possible.\n",
    "\n",
    "When setting up sequences, there are two dynamic axes that are important to consider. The first is the batch axis, which is the axis along which multiple sequences are batched. The second is the dynamic axis particular to that sequence. The latter is specific to a particular input because of variable sequence lengths in your data. In CNTK, we use the dynamic axe name to idenitify different dynamic axes, and all sequence oprations between different variables require them have the same dynamic axes which means they must have the same length on all the axes. \n",
    "\n",
    "In ReasoNet networks, we have five input streams/sequences: *query*, *paragraph*, *label*, *entity id*, *entity indicator*, where *entity id* and *entity indicator* are helper sequences. *Query*, *paragraph* and *entity id* have different sequence lengths so they have different sequence dynamic axis. *label* and *entity id* have the same sequence length as *paragraph*, so they share the same dynamic axis. As a result, there will be two sequence_axes, *sourceAxis* and *contextAxis* when we create the model. The we can define input *query_sequence* over *sourceAxis*, *context_sequence* and *entity_ids_mask* over *contextAxis*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_model(params):\n",
    "  \"\"\"\n",
    "  Create ReasoNet model\n",
    "  Args:\n",
    "    params (class:`model_params`): The parameters used to create the model\n",
    "  \"\"\"\n",
    "  logger.log(\"Create model: dropout_rate: {0}, init:{1}, embedding_init: {2}\"\\\n",
    "             .format(params.dropout_rate, params.init, params.embedding_init))\n",
    "    \n",
    "  # Query and Doc/Context/Paragraph inputs to the model\n",
    "  query_seq_axis = Axis('sourceAxis')\n",
    "  context_seq_axis = Axis('contextAxis')\n",
    "\n",
    "  # Specify the query and the context sequence inputs\n",
    "  query_sequence = sequence.input(shape=(params.vocab_dim), \n",
    "                                  is_sparse=True, \n",
    "                                  sequence_axis=query_seq_axis, \n",
    "                                  name='query')\n",
    "  context_sequence = sequence.input(shape=(params.vocab_dim), \n",
    "                                    is_sparse=True, \n",
    "                                    sequence_axis=context_seq_axis, \n",
    "                                    name='context')\n",
    "  \n",
    "  # Specify the container to hold entity ids mask with the same length as context sequence \n",
    "  # where each iterm is and indicator of whether the corresponding word in the context is an entity or not.\n",
    "  entity_ids_mask = sequence.input(shape=(1,), \n",
    "                                   is_sparse=False, \n",
    "                                   sequence_axis=context_seq_axis, \n",
    "                                   name='entity_ids_mask')\n",
    "    \n",
    "  # Either random initialize the embedding or load a pre-dertermined embedding (say GLOVE)  \n",
    "  if params.embedding_init is None:\n",
    "    embedding_init = create_random_matrix(params.vocab_dim, params.embedding_dim)\n",
    "  else:\n",
    "    embedding_init = params.embedding_init\n",
    "  embedding = parameter(shape=(params.vocab_dim, params.embedding_dim), init=None)\n",
    "  embedding.value = embedding_init\n",
    "  constant_embedding = constant(embedding_init, shape=(params.vocab_dim, params.embedding_dim))\n",
    "\n",
    "  # Optionally add dropouts in the embeddings\n",
    "  if params.dropout_rate is not None:\n",
    "    query_embedding  = ops.dropout(times(query_sequence , embedding), \n",
    "                                   params.dropout_rate, \n",
    "                                   name='query_embedding')\n",
    "    context_embedding = ops.dropout(times(context_sequence, embedding), \n",
    "                                    params.dropout_rate, \n",
    "                                    name='context_embedding')\n",
    "  else:\n",
    "    query_embedding  = times(query_sequence , embedding, name='query_embedding')\n",
    "    context_embedding = times(context_sequence, embedding, name='context_embedding')\n",
    "    \n",
    "  \n",
    "  # Create containers for the GRU weights for query and context \n",
    "  context_gru_weights = Parameter(_INFERRED + (params.hidden_dim,), \n",
    "                                  init=glorot_uniform(), \n",
    "                                  name='context_gru_params')\n",
    "  if params.share_rnn_param:\n",
    "    query_gru_weights = context_gru_weights\n",
    "  else:\n",
    "    query_gru_weights = Parameter(_INFERRED + (params.hidden_dim,), init=glorot_uniform(), name='query_gru_params')\n",
    "\n",
    "  # We use constant random vectors as the embedding of entities in the paragraph, \n",
    "  # as we treat them as meaningless symbolic in the paragraph which is equal to entity shuffle\n",
    "  entity_embedding = ops.times(context_sequence, constant_embedding, name='constant_entity_embedding')\n",
    "  \n",
    "  # Unlike other words in the context, \n",
    "  # we keep the entity vectors fixed as a random vector so that each vector just means an identifier \n",
    "  # of different entities in the context and it has no semantic meaning\n",
    "  full_context_embedding = ops.element_select(entity_ids_mask, entity_embedding, context_embedding)\n",
    "  context_memory = ops.optimized_rnnstack(full_context_embedding, \n",
    "                                          context_gru_weights, \n",
    "                                          params.hidden_dim, \n",
    "                                          num_layers=1, \n",
    "                                          bidirectional=True, recurrent_op='gru', name='context_mem')\n",
    "\n",
    "  query_memory = ops.optimized_rnnstack(query_embedding, \n",
    "                                        query_gru_weights, \n",
    "                                        params.hidden_dim, \n",
    "                                        num_layers=1, \n",
    "                                        bidirectional=True, recurrent_op='gru', name='query_mem')\n",
    "    \n",
    "  qfwd = ops.slice(sequence.last(query_memory), -1, 0, params.hidden_dim, name='fwd')\n",
    "  qbwd = ops.slice(sequence.first(query_memory), -1, params.hidden_dim, params.hidden_dim*2, name='bwd')\n",
    "  init_status = ops.splice(qfwd, qbwd, name='Init_Status') # get last fwd status and first bwd status\n",
    "\n",
    "  return reasonet_model(context_memory, query_memory, init_status, params.hidden_dim, \n",
    "                         params.attention_dim, max_steps = params.max_rl_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Loss function (Contrastive Reward)\n",
    "\n",
    "One of the signiﬁcant challenges ReasoNet faces is how to design an efﬁcient training method, since the termination state is discrete and not connected to the ﬁnal output. This prohibits the canonical back-propagation method from being directly applied to train ReasoNet. This challenge is tackled by a novel deep reinforcement learning method called Contrastive Reward (CR).\n",
    "\n",
    "Unlike traditional reinforcement learning optimization methods using a global variable to capture rewards, CR utilizes an instance-based reward baseline assignment. Experiments show the superiority of CR in both training speed and accuracy. \n",
    "\n",
    "Finally, by accounting for a dynamic termination state during inference and applying the proposed deep reinforcement learning optimization method, ReasoNet can achieve the state-of-the-art results in machine comprehension datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def contrastive_reward(labels, predictions_and_stop_probabilities):\n",
    "  \"\"\"\n",
    "  Compute the contrastive reward loss in paper 'ReasoNet: \n",
    "    Learning to Stop Reading in Machine Comprehension'\n",
    "  Args:\n",
    "    labels: The lables\n",
    "    predictions_and_stop_probabilities: A list of tuples, \n",
    "    each tuple contains the prediction and stop probability of the coresponding step.\n",
    "  \"\"\"\n",
    "  base = None\n",
    "  avg_rewards = None\n",
    "  for step in range(len(predictions_and_stop_probabilities)):\n",
    "    pred = predictions_and_stop_probabilities[step][0]\n",
    "    stop = predictions_and_stop_probabilities[step][1]\n",
    "    if base is None:\n",
    "      base = ops.element_times(pred, stop)\n",
    "    else:\n",
    "      base = ops.plus(ops.element_times(pred, stop), base)\n",
    "    \n",
    "  avg_rewards = ops.stop_gradient(sequence.reduce_sum(base*labels))\n",
    "  base_reward = sequence.broadcast_as(avg_rewards, base, name = 'base_line')\n",
    "    \n",
    "  # While  the learner will mimize the loss by default, we want it to maximize the rewards\n",
    "  # Maximum rewards => minimal -rewards\n",
    "  # So we use (1-r/b) as the rewards instead of (r/b-1)\n",
    "  step_cr = ops.stop_gradient(1- ops.element_divide(labels, base_reward))\n",
    "  normalized_contrastive_rewards = ops.element_times(base, step_cr)\n",
    "  rewards = sequence.reduce_sum(normalized_contrastive_rewards) + avg_rewards\n",
    "  return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Loss and accuracy\n",
    "\n",
    "At loss computation stage, we will consume another two inputs, *enity_ids* and *labels*. They have the same length and share the same dynamic axes which is created programatically (see the comments inline the code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def accuracy_func(prediction, label, name='accuracy'):\n",
    "  \"\"\"\n",
    "  Compute the accuracy of the prediction\n",
    "  \"\"\"\n",
    "  pred_max = ops.hardmax(prediction, name='pred_max')\n",
    "  norm_label = ops.equal(label, [1], name='norm_label')\n",
    "  acc = ops.times_transpose(pred_max, norm_label, name='accuracy')\n",
    "  return acc\n",
    "\n",
    "def loss(model, params):\n",
    "  \"\"\"\n",
    "  Compute the loss and accuracy of the model output\n",
    "  \"\"\"\n",
    "  model_args = {arg.name:arg for arg in model.arguments}\n",
    "  context = model_args['context']\n",
    "  entity_ids_mask = model_args['entity_ids_mask']\n",
    "  entity_condition = greater(entity_ids_mask, 0, name='condidion')\n",
    "\n",
    "  # Get all the enities in the paragraph via gather operator, which will create a new dynamic sequence axis \n",
    "  entities_all = sequence.gather(entity_condition, entity_condition, name='entities_all')\n",
    "\n",
    "  # The generated dynamic axis has the same length as the input enity id sequence, \n",
    "  # so we asign it as the entity id's dynamic axis.\n",
    "  entity_ids = input(shape=(params.entity_dim), is_sparse=True, \n",
    "                              dynamic_axes=entities_all.dynamic_axes, name='entity_ids')\n",
    "    \n",
    "  wordvocab_dim = params.vocab_dim\n",
    "  labels_raw = input(shape=(1,), is_sparse=False, dynamic_axes=context.dynamic_axes, \n",
    "                              name='labels')\n",
    "    \n",
    "  answers = sequence.scatter(sequence.gather(model.outputs[-1], entity_condition), entities_all, name='Final_Ans')\n",
    "  labels = sequence.scatter(sequence.gather(labels_raw, entity_condition), entities_all, name='EntityLabels')\n",
    "  entity_id_matrix = ops.reshape(entity_ids, params.entity_dim)\n",
    "  \n",
    "  # We aggragate the scores of the same enities at different position in \n",
    "  # the paragraph as the final score of the entity\n",
    "  aggregated_pred = sequence.reduce_sum(element_times(answers, entity_id_matrix))\n",
    "  aggregated_label = ops.greater_equal(sequence.reduce_sum(element_times(labels, entity_id_matrix)), 1)\n",
    "  predictions_and_stop_probabilities=[]\n",
    "\n",
    "  for step in range(int((len(model.outputs)-1)/2)):\n",
    "    predictions_and_stop_probabilities += [(model.outputs[step*2], model.outputs[step*2+1])]\n",
    "    \n",
    "  loss_value = contrastive_reward(labels_raw, predictions_and_stop_probabilities)\n",
    "  accuracy = accuracy_func(aggregated_pred, aggregated_label, name='accuracy')\n",
    "  apply_loss = combine([loss_value, answers, labels, accuracy], name='Loss')\n",
    "\n",
    "  return apply_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Adam Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_adam_learner(learn_params, learning_rate = 0.0005, gradient_clipping_threshold_per_sample=0.001):\n",
    "  \"\"\"\n",
    "  Create adam learner\n",
    "  \"\"\"\n",
    "  lr_schedule = learners.learning_rate_schedule(learning_rate, learners.UnitType.sample)\n",
    "  momentum = learners.momentum_schedule(0.90)\n",
    "  gradient_clipping_threshold_per_sample = gradient_clipping_threshold_per_sample\n",
    "  gradient_clipping_with_truncation = True\n",
    "  momentum_var = learners.momentum_schedule(0.999)\n",
    "  lr = learners.adam(learn_params, lr_schedule, momentum, True, momentum_var,\n",
    "          gradient_clipping_threshold_per_sample = gradient_clipping_threshold_per_sample,\n",
    "          gradient_clipping_with_truncation = gradient_clipping_with_truncation)\n",
    "  learner_desc = 'Alg: Adam, learning rage: {0}, momentum: {1}, gradient clip: {2}'\\\n",
    "    .format(learning_rate, momentum[0], gradient_clipping_threshold_per_sample)\n",
    "  logger.log(\"Create learner. {0}\".format(learner_desc))\n",
    "  return lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def __evaluation(trainer, data, bind, minibatch_size, epoch_size):\n",
    "  \"\"\"\n",
    "  Evaluate the loss and accurate of the evaluation data set during training stage\n",
    "  \"\"\"\n",
    "  if epoch_size is None:\n",
    "    epoch_size = 1\n",
    "  context_stream = get_context_bind_stream(bind)\n",
    "  eval_acc = 0\n",
    "  eval_s = 0\n",
    "  k = 0\n",
    "  print(\"Start evaluation with {0} samples ...\".format(epoch_size))\n",
    "  while k < epoch_size:\n",
    "    mbs = min(epoch_size - k, minibatch_size)\n",
    "    mb = data.next_minibatch(mbs, input_map=bind)\n",
    "    k += mb[context_stream].num_samples\n",
    "    sm = mb[context_stream].num_sequences\n",
    "    avg_acc = trainer.test_minibatch(mb)\n",
    "    eval_acc += sm*avg_acc\n",
    "    eval_s += sm\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "  eval_acc /= eval_s\n",
    "  print(\"\")\n",
    "  logger.log(\"Evaluation Acc: {0}, samples: {1}\".format(eval_acc, eval_s))\n",
    "  return eval_acc\n",
    "\n",
    "def train(model, m_params, learner, train_data, max_epochs=1, \n",
    "          save_model_flag=False, epoch_size=270000, eval_data=None, eval_size=None, \n",
    "          check_point_freq=0.1, minibatch_size=50000, model_name='rsn'):\n",
    "  \"\"\"\n",
    "  Train the model\n",
    "  Args:\n",
    "    model: The created model\n",
    "    m_params: Model parameters\n",
    "    learner: The learner used to train the model\n",
    "  \"\"\"\n",
    "  criterion_loss = loss(model, m_params)\n",
    "  loss_func = criterion_loss.outputs[0]\n",
    "  eval_func = criterion_loss.outputs[-1]\n",
    "  trainer = Trainer(model.outputs[-1], (loss_func, eval_func), learner)\n",
    "\n",
    "  # Get minibatches of sequences to train with and perform model training\n",
    "  # bind inputs to data from readers\n",
    "  train_bind = bind_data(criterion_loss, train_data)\n",
    "  context_stream = get_context_bind_stream(train_bind)\n",
    "  eval_bind = bind_data(criterion_loss, eval_data)\n",
    "\n",
    "  i = 0\n",
    "  minibatch_count = 0\n",
    "  training_progress_output_freq = 500\n",
    "  check_point_interval = int(epoch_size*check_point_freq)\n",
    "  check_point_id = 0\n",
    "  for epoch in range(max_epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_samples = 0\n",
    "    i = 0\n",
    "    win_loss = 0\n",
    "    win_acc = 0\n",
    "    win_samples = 0\n",
    "    chk_loss = 0\n",
    "    chk_acc = 0\n",
    "    chk_samples = 0\n",
    "    while i < epoch_size:\n",
    "      # get next minibatch of training data\n",
    "      mbs = min(minibatch_size, epoch_size - i)\n",
    "      mb_train = train_data.next_minibatch(minibatch_size, input_map=train_bind)\n",
    "      i += mb_train[context_stream].num_samples\n",
    "      trainer.train_minibatch(mb_train)\n",
    "      minibatch_count += 1\n",
    "      sys.stdout.write('.')\n",
    "      sys.stdout.flush()\n",
    "      # collect epoch-wide stats\n",
    "      samples = trainer.previous_minibatch_sample_count\n",
    "      ls = trainer.previous_minibatch_loss_average * samples\n",
    "      acc = trainer.previous_minibatch_evaluation_average * samples\n",
    "      epoch_loss += ls\n",
    "      epoch_acc += acc\n",
    "      win_loss += ls\n",
    "      win_acc += acc\n",
    "      chk_loss += ls\n",
    "      chk_acc += acc\n",
    "      epoch_samples += samples\n",
    "      win_samples += samples\n",
    "      chk_samples += samples\n",
    "      if int(epoch_samples/training_progress_output_freq) != \\\n",
    "        int((epoch_samples-samples)/training_progress_output_freq):\n",
    "        print('')\n",
    "        logger.log(\"Lastest sample count = {}, Train Loss: {}, Evalualtion ACC: {}\"\\\n",
    "                   .format(win_samples, win_loss/win_samples,\n",
    "          win_acc/win_samples))\n",
    "        logger.log(\"Total sample count = {}, Train Loss: {}, Evalualtion ACC: {}\"\\\n",
    "                   .format(chk_samples, chk_loss/chk_samples,\n",
    "          chk_acc/chk_samples))\n",
    "        win_samples = 0\n",
    "        win_loss = 0\n",
    "        win_acc = 0\n",
    "      new_chk_id = int(i/check_point_interval)\n",
    "      if new_chk_id != check_point_id and i < epoch_size :\n",
    "        check_point_id = new_chk_id\n",
    "        print('')\n",
    "        logger.log(\"--- CHECKPOINT %d: samples=%d, loss = %.2f, acc = %.2f%% ---\" % (check_point_id, \n",
    "                                                                                     chk_samples, \n",
    "                                                                                     chk_loss/chk_samples, \n",
    "                                                                                     100.0*(chk_acc/chk_samples)))\n",
    "        if eval_data:\n",
    "          __evaluation(trainer, eval_data, eval_bind, minibatch_size, eval_size)\n",
    "        if save_model_flag:\n",
    "          # save the model every epoch\n",
    "          model_filename = os.path.join('model', \"model_%s_%02d_%03d.dnn\" % (model_name, epoch, check_point_id))\n",
    "          model.save_model(model_filename)\n",
    "          logger.log(\"Saved model to '%s'\" % model_filename)\n",
    "        chk_samples = 0\n",
    "        chk_loss = 0\n",
    "        chk_acc = 0\n",
    "\n",
    "    print('')\n",
    "    logger.log(\"--- EPOCH %d: samples=%d, loss = %.2f, acc = %.2f%% ---\" % (epoch, epoch_samples,\n",
    "                                                                            epoch_loss/epoch_samples,\n",
    "                                                                            100.0*(epoch_acc/epoch_samples)))\n",
    "  eval_acc = 0\n",
    "  if eval_data:\n",
    "    eval_acc = __evaluation(trainer, eval_data, eval_bind, minibatch_size, eval_size)\n",
    "  if save_model_flag:\n",
    "    # save the model every epoch\n",
    "    model_filename = os.path.join('model', \"model_%s_final.dnn\" % (model_name))\n",
    "    model.save_model(model_filename)\n",
    "    logger.log(\"Saved model to '%s'\" % model_filename)\n",
    "  return (epoch_loss/epoch_samples, epoch_acc/epoch_samples, eval_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train the model for CNN data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log with log file: log/cnn_model_04-15_19.10.56.log\n",
      "Create model: dropout_rate: 0.2, init:<cntk.cntk_py.Dictionary; proxy of <Swig Object of type 'CNTK::ImageTransform *' at 0x7fc5300c1480> >, embedding_init: None\n",
      "Create learner. Alg: Adam, learning rage: 0.0005, momentum: 0.9, gradient clip: 0.001\n",
      ".........\n",
      "Lastest sample count = 564, Train Loss: 0.010489991899077775, Evalualtion ACC: 0.3102836879432624\n",
      "Total sample count = 564, Train Loss: 0.010489991899077775, Evalualtion ACC: 0.3102836879432624\n",
      ".......\n",
      "Lastest sample count = 460, Train Loss: 0.04828313031922216, Evalualtion ACC: 0.30217391304347824\n",
      "Total sample count = 1024, Train Loss: 0.02746737829875201, Evalualtion ACC: 0.306640625\n",
      "........\n",
      "Lastest sample count = 515, Train Loss: 0.10321561942980127, Evalualtion ACC: 0.2524271844660194\n",
      "Total sample count = 1539, Train Loss: 0.0528152302691811, Evalualtion ACC: 0.2884990253411306\n",
      "........\n",
      "Lastest sample count = 519, Train Loss: 0.1334948080353195, Evalualtion ACC: 0.2909441233140655\n",
      "Total sample count = 2058, Train Loss: 0.07316153778163291, Evalualtion ACC: 0.2891156462585034\n",
      "\n",
      "--- EPOCH 0: samples=2058, loss = 0.07, acc = 28.91% ---\n",
      "Start evaluation with 392389 samples ...\n",
      "........\n",
      "Evaluation Acc: 0.336, samples: 500\n",
      "Saved model to 'model/model_training.ctf_final.dnn'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import cntk.device as device\n",
    "import numpy as np\n",
    "from cntk.ops.tests.ops_test_utils import cntk_device\n",
    "from cntk.ops import input_variable, past_value, future_value\n",
    "from cntk.io import MinibatchSource\n",
    "from cntk import Trainer, Axis, device, combine\n",
    "from cntk.layers import Recurrence, Convolution\n",
    "import cntk.ops as ops\n",
    "import cntk\n",
    "import math\n",
    "\n",
    "def train_reasonet_cnn():\n",
    "  logger.init(\"cnn_model\")\n",
    "  data_path = train_ctf\n",
    "  eval_path = validation_ctf\n",
    "  vocab_dim = 101585\n",
    "  entity_dim = 586\n",
    "  epoch_size=289716292\n",
    "  eval_size=2993016\n",
    "  hidden_dim=256\n",
    "  max_rl_steps=5\n",
    "  max_epochs=5\n",
    "  embedding_dim=300\n",
    "  att_dim = 384\n",
    "  # The average sequence length is about 700, so we set the minibatch_size to 50000 in sequence num,\n",
    "  # which is about 70 samples/instanes per minibatch\n",
    "  minibatch_size = 50000 \n",
    "  check_point_freq = 0.1\n",
    "  train_data = create_reader(data_path, vocab_dim, entity_dim, True)\n",
    "  eval_data = create_reader(eval_path, vocab_dim, entity_dim, False) \\\n",
    "    if eval_path is not None else None\n",
    "    \n",
    "  glove_path = os.path.join(data_root, \"glove/glove.6B.{0}d.txt\".format(embedding_dim))  \n",
    "  scale = math.sqrt(6/(vocab_dim+embedding_dim))*2\n",
    "  init = uniform_initializer(scale, -scale/2)\n",
    "  embedding_init = load_embedding(glove_path, vocab_path, embedding_dim, init) if os.path.exists(glove_path) \\\n",
    "    else None\n",
    "    \n",
    "  demo_only=True\n",
    "  if is_test():\n",
    "    minibatch_size=50000\n",
    "    hidden_dim = 128\n",
    "    att_dim = 128\n",
    "    max_rl_steps = 2\n",
    "    max_epochs = 1\n",
    "    embedding_dim=100\n",
    "    embedding_init = None\n",
    "    epoch_size=train_size\n",
    "    eval_size=validation_size\n",
    "    check_point_freq = 1\n",
    "  elif demo_only:\n",
    "    # Use a smaller minibatch_size to reduce memory usage for demo popurse only\n",
    "    minibatch_size=1000\n",
    "    hidden_dim = 128\n",
    "    att_dim = 128\n",
    "    max_rl_steps = 2\n",
    "    max_epochs = 1\n",
    "    embedding_dim=100\n",
    "    embedding_init = None\n",
    "    epoch_size=28971\n",
    "    eval_size=29930\n",
    "    check_point_freq = 1\n",
    "    \n",
    "  params = model_params(vocab_dim = vocab_dim, entity_dim = entity_dim, hidden_dim = hidden_dim, \n",
    "                        embedding_dim = embedding_dim, attention_dim=att_dim, max_rl_steps = max_rl_steps,\n",
    "                        embedding_init = embedding_init, dropout_rate = 0.2, share_rnn_param=True)\n",
    "\n",
    "  model = create_model(params)\n",
    "  learner = create_adam_learner(model.parameters)\n",
    "  (train_loss, train_acc, eval_acc) = train(model, params, learner, train_data, \n",
    "                                            max_epochs=max_epochs, epoch_size=epoch_size, \n",
    "                                            save_model_flag=True, model_name=os.path.basename(data_path),\n",
    "                                            eval_data=eval_data, eval_size=eval_size, check_point_freq=check_point_freq,\n",
    "                                            minibatch_size = minibatch_size)\n",
    "\n",
    "train_reasonet_cnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Test the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Get model to local\n",
    "Due to the long training proecess, we have uploaded a trained model to a public website. To evalute it, we need to download it to local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succeeded to download model to local.\n"
     ]
    }
   ],
   "source": [
    "if is_test():\n",
    "  model_src=\"http://cntk.ai/jup/models/reasonet/model_training.ctf_final.dnn.bin\"\n",
    "  model_path=\"model/model_training.ctf_final.dnn\"\n",
    "else:\n",
    "  model_src=\"http://cntk.ai/jup/models/reasonet/model_cnn.epoch.00.bin\"\n",
    "  model_path=\"model/model_cnn.epoch.00.bin\"\n",
    "if not file_exists(model_path):\n",
    "  download(model_src, model_path)\n",
    "    \n",
    "print(\"Succeeded to download model to local.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Test the model with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log with log file: log/cnn_test_04-15_19.13.05.log\n",
      "........\n",
      "Evaluation Acc: 0.32, loss: 0.1614980888366699, samples: 500 in 9.912116765975952 seconds\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import cntk.device as device\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from cntk import load_model\n",
    "\n",
    "def test_cnn_model(model_path):\n",
    "  logger.init(\"cnn_test\")\n",
    "  test_path = test_ctf\n",
    "  vocab_dim = 101585\n",
    "  entity_dim = 586\n",
    "  hidden_dim=256\n",
    "  max_rl_steps=5\n",
    "  embedding_dim=300\n",
    "  att_dim = 384\n",
    "  minibatch_size=50000\n",
    "  share_rnn = True\n",
    "\n",
    "  test_data = create_reader(test_path, vocab_dim, entity_dim, False)\n",
    "  embedding_init = None\n",
    "\n",
    "  params = model_params(vocab_dim = vocab_dim, entity_dim = entity_dim, hidden_dim = hidden_dim,\n",
    "                        embedding_dim = embedding_dim, attention_dim=att_dim, max_rl_steps = max_rl_steps,\n",
    "                        embedding_init = embedding_init, dropout_rate = 0.2, share_rnn_param = share_rnn)\n",
    "\n",
    "  model = load_model(model_path)\n",
    "  loss_func = loss(model, params)\n",
    "  bind = bind_data(loss_func, test_data)\n",
    "  context_stream = get_context_bind_stream(bind)\n",
    "  loss_sum = 0\n",
    "  acc_sum = 0\n",
    "  samples_sum = 0\n",
    "  i = 0\n",
    "  start = time.time()\n",
    "  while i<test_size:\n",
    "    mbs = min(test_size - i, minibatch_size)\n",
    "    mb = test_data.next_minibatch(mbs, bind)\n",
    "    outs = loss_func.eval(mb)\n",
    "    loss_value = np.sum(outs[loss_func.outputs[0]])\n",
    "    acc = np.sum(outs[loss_func.outputs[-1]])\n",
    "    i += mb[context_stream].num_samples\n",
    "    samples = mb[context_stream].num_sequences\n",
    "    samples_sum += samples\n",
    "    acc_sum += acc\n",
    "    loss_sum += loss_value\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "  end = time.time()\n",
    "  total = end - start\n",
    "  print(\"\")\n",
    "  print(\"Evaluation Acc: {0}, loss: {1}, samples: {2} in {3} seconds\".format(acc_sum/samples_sum, loss_sum/samples_sum, samples_sum, total))\n",
    "\n",
    "test_cnn_model(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### Appendix\n",
    "#### Mathematical details from the original paper\n",
    "\n",
    "\n",
    "\n",
    "In the ReasoNet paper, it gives the fomula of the Reward as\n",
    "\\begin{align}\n",
    "J(\\theta) = \\mathbf{E}_{\\pi\\left(t_{1:T},a_T;\\theta\\right)}\\left[\\sum_{t=1}^Tr_t\\right]\n",
    "\\end{align}\n",
    "\n",
    "And it applies REINFORCE algorithm to estimate \n",
    "\\begin{align} \n",
    "\\nabla_{\\theta}J(\\theta) = \\mathbf{E}_{\\pi\\left(t_{1:T},a_T;\\theta\\right)}\\left[\\nabla_{\\theta}log_{\\pi}\\left(t_{1:T},a_T;\\theta\\right)r_T\\right]=\\sum_{\\left(t_{1:T},a_T\\right)\\in\\mathbb{A}^+}\\pi\\left(t_{1:T},a_T;\\theta\\right)\\left[\\nabla_{\\theta}log\\pi\\left(t_{1:T},a_T;\\theta\\right)\\left(r_T-b_T\\right)\\right]\n",
    "\\end{align}\n",
    "\n",
    "However, as the baseline $\\left\\{b_T;T=1...T_{max}\\right\\}$ are global variables independent of instances, it leads to slow convergence in training ReasoNet. Instead, the paper rewrite the formular as,\n",
    "$$\n",
    "\\nabla_{\\theta}J(\\theta) =\\sum_{\\left(t_{1:T},a_T\\right)\\in\\mathbb{A}^+}\\pi\\left(t_{1:T},a_T;\\theta\\right)\\left[\\nabla_{\\theta}log\\pi\\left(t_{1:T},a_T;\\theta\\right)\\left(r_T-b\\right)\\right]\n",
    "$$\n",
    ",where $b=\\sum_{\\left(t_{1:T},a_T\\right)\\in\\mathbb{A}^+}\\pi\\left(t_{1:T},a_T;\\theta\\right)r_T$ is the average reward on the $\\left|\\mathbb{A}^+\\right|$ episodes.\n",
    "\n",
    "Since the sum of the rewards over $\\left|\\mathbb{A}^+\\right|$ episodes is zero, $\\sum_{\\left(t_{1:T},a_T\\right)\\in\\mathbb{A}^+}\\pi\\left(t_{1:T},a_T;\\theta\\right)\\left(r_T-b\\right)=0$, they call it Contrastive Reward. Further more, they found using $\\left(\\frac{r_T}{b}-1\\right)$ in replace of $\\left(r_T-b\\right)$ will lead to a better convergence.\n",
    "\n",
    "In our implementation, we take the reward in the form,\n",
    "$$\n",
    "J(\\theta)=\\sum_{\\left(t_{1:T},a_T\\right)\\in\\mathbb{A}^+}\\pi\\left(t_{1:T},a_T;\\theta\\right)\\left(\\frac{r_T}{b}-1\\right) + b\n",
    "$$\n",
    "As we only compute gradient on $\\pi\\left(t_{1:T},a_T;\\theta\\right)$ and treat other components in the formula as a constant, the derivate is the same as the paper while the output is the average rewards in $\\left|\\mathbb{A}^+\\right|$ episodes.\n",
    "In CNTK, we use stop_gradient operator over the output of a function to convert it to a constant in the math formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
