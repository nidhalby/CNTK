{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# CNTK 302b: Evaluation ReasoNet for Machine Comprehension with CNN Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This tutorial loads a pre-trained ReasoNet model and shows how cached models can be used to perform predictions a.k.a evalation on CNN data set that was not used in the training.\n",
    "\n",
    "## Data preparation\n",
    "\n",
    "### Download data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All necessary data are downloaded to ../Examples/LanguageUnderstanding/ReasoNet/Data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../Examples/LanguageUnderstanding/\")\n",
    "from ReasoNet.prepare_cnn_data import file_exists,merge_files,download_cnn,download\n",
    "\n",
    "# Check for an environment variable defined in CNTK's test infrastructure\n",
    "envvar = 'CNTK_EXTERNAL_TESTDATA_SOURCE_DIRECTORY'\n",
    "def is_test(): \n",
    "  #return envvar in os.environ\n",
    "  return True\n",
    "    \n",
    "data_root = \"../Examples/LanguageUnderstanding/ReasoNet/Data\"\n",
    "\n",
    "if is_test():\n",
    "  raw_train_data=os.path.join(data_root, \"cnn_test/training.txt\")\n",
    "  raw_test_data=os.path.join(data_root, \"cnn_test/test.txt\")\n",
    "else:\n",
    "  raw_train_data=os.path.join(data_root, \"cnn/training.txt\")\n",
    "  raw_test_data=os.path.join(data_root, \"cnn/test.txt\")\n",
    "  if not (file_exists(raw_train_data) and file_exists(raw_test_data)):\n",
    "    download_cnn(data_root)\n",
    "  merge_files(os.path.join(data_root, \"cnn/questions/training\"), raw_train_data)\n",
    "  merge_files(os.path.join(data_root, \"cnn/questions/test\"), raw_test_data)\n",
    "print(\"All necessary data are downloaded to {0}\".format(data_root))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convert to CNTK Text Format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data conversion finished.\n"
     ]
    }
   ],
   "source": [
    "from ReasoNet.wordvocab import *\n",
    "\n",
    "if is_test():\n",
    "  vocab_path=os.path.join(data_root, \"cnn_test/cnn.vocab\")\n",
    "  train_ctf=os.path.join(data_root, \"cnn_test/training.ctf\")\n",
    "  test_ctf=os.path.join(data_root, \"cnn_test/test.ctf\")\n",
    "  test_size=379913\n",
    "else:\n",
    "  vocab_path=os.path.join(data_root, \"cnn/cnn.vocab\")\n",
    "  train_ctf=os.path.join(data_root, \"cnn/training.ctf\")\n",
    "  test_ctf=os.path.join(data_root, \"cnn/test.ctf\")\n",
    "  test_size=2291183\n",
    "vocab_size=101000\n",
    "if not (file_exists(train_ctf) and file_exists(test_ctf)):\n",
    "  entity_vocab, word_vocab = Vocabulary.build_vocab(raw_train_data, vocab_path, vocab_size)\n",
    "  Vocabulary.build_corpus(entity_vocab, word_vocab, raw_test_data, test_ctf)\n",
    "print(\"Data conversion finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succeeded to download model to local.\n"
     ]
    }
   ],
   "source": [
    "if is_test():\n",
    "  model_src=\"http://cntk.ai/jup/models/reasonet/model_training.ctf_final.dnn.bin\"\n",
    "  model_path=\"model/model_training.ctf_final.dnn\"\n",
    "else:\n",
    "  model_src=\"http://cntk.ai/jup/models/reasonet/model_cnn.epoch.00.bin\"\n",
    "  model_path=\"model/model_cnn.epoch.00.bin\"\n",
    "if not file_exists(model_path):\n",
    "  download(model_src, model_path)\n",
    "    \n",
    "print(\"Succeeded to download model to local.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Basic CNTK imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import cntk\n",
    "from cntk import device\n",
    "from cntk.ops import sequence, element_times, reshape, greater, slice, hardmax, input\n",
    "from io import open\n",
    "\n",
    "# Select the right target device when this notebook is being tested\n",
    "# Currently supported only for GPU \n",
    "\n",
    "if 'TEST_DEVICE' in os.environ:\n",
    "    if os.environ['TEST_DEVICE'] == 'cpu':\n",
    "        raise ValueError('This notebook is currently not support on CPU') \n",
    "    else:\n",
    "        cntk.device.set_default_device(cntk.device.gpu(0))\n",
    "cntk.device.set_default_device(cntk.device.gpu(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Predict\n",
    "The original CNN data has been pre-processed by replace entities in the text with *@entityXX* and the answer is taken from the entities. Here is an example,\n",
    "\n",
    "* The original paragraph,\n",
    ">april 2 , 2015 an unstable Middle Eastern country has become a potential battlefield for a proxy war . today on CNN Student News , hear an explainer on why Yemen is the focus of global concern . we also report on the origins of April Fools ' Day , we detail how a 1,000 - year - old recipe could cure a modern - day superbug , and we feature a Character Study on a woman who 's steering kids to a better life . on this page you will find today 's show transcript and a place for you to request to be on the CNN Student News Roll Call . transcript click here to access the transcript of today 's CNN Student News program . please note that there may be a delay between the time when the video is available and when the transcript is published . CNN Student News is created by a team of journalists who consider the Common Core State Standards , national standards in different subject areas , and state standards when producing the show . ROLL CALL for a chance to be mentioned on the next CNN Student News , comment on the bottom of this page with your school name , mascot , city and state . we will be selecting schools from the comments of the previous show . you must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call ! thank you for using CNN student news !\n",
    "\n",
    "* The original query,\n",
    ">at the bottom of the page , comment for a chance to be mentioned on CNN Student News . you must be a teacher or a student age 13 or older to request a mention on the @placeholder .\n",
    "\n",
    "* The answer\n",
    ">CNN Student News Roll Call\n",
    "\n",
    "After pre-processing, it will be looks like,\n",
    "\n",
    "* Paragraph\n",
    ">april 2 , 2015 an unstable @entity1 country has become a potential battlefield for a proxy war . today on @entity4 , hear an explainer on why @entity6 is the focus of global concern . we also report on the origins of @entity11 , we detail how a 1,000 - year - old recipe could cure a modern - day superbug , and we feature a @entity14 on a woman who 's steering kids to a better life . on this page you will find today 's show transcript and a place for you to request to be on the @entity22 . transcript click here to access the transcript of today 's @entity25 . please note that there may be a delay between the time when the video is available and when the transcript is published . @entity4 is created by a team of journalists who consider the @entity33 , national standards in different subject areas , and state standards when producing the show . @entity38 for a chance to be mentioned on the next @entity4 , comment on the bottom of this page with your school name , mascot , city and state . we will be selecting schools from the comments of the previous show . you must be a teacher or a student age 13 or older to request a mention on the @entity22 ! thank you for using @entity56 student news !\n",
    "\n",
    "* Query\n",
    ">at the bottom of the page , comment for a chance to be mentioned on @entity4 . you must be a teacher or a student age 13 or older to request a mention on the @placeholder .\n",
    "\n",
    "* Answer\n",
    ">@entity22\n",
    "\n",
    "After we get the model, we can use it to predict answers given a paragraph and a query. The inputs to our `predict` function is the *pre-processed* paragraphs and queries. The output is a one hot vector whose dimention is the number of **unique** entities in the paragraph as we pick answer from the entities in the paragraph. And a 1 in the vector means the entity at that position(*the position index is the same as entity id*) is the predicted answer and 0 means not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from ReasoNet.reasonet import *\n",
    "def predict(model, params):\n",
    "  \"\"\"\n",
    "  Compute the prediction result of the given model\n",
    "  \"\"\"\n",
    "  model_args = {arg.name:arg for arg in model.arguments}\n",
    "    \n",
    "  # entity_ids_mask is a sequence of boolean with the same length \n",
    "  #  as the number of tokens in the paragraph, \n",
    "  #  where none zero means the corresponding token is an entity.\n",
    "  # E.g.\n",
    "  # Paragraph\n",
    "  # Abc efg @entity1 xyz @entity1 @entity3\n",
    "  # The corresponding entity ids mask:\n",
    "  #  0  0  1  0  1  1\n",
    "  entity_ids_mask = model_args['entity_ids_mask']\n",
    "  \n",
    "  # Normalize the input to make all none zero values to 1s\n",
    "  entity_condition = greater(entity_ids_mask, 0, name='condidion')\n",
    "    \n",
    "  # entities_all is sequence of all 1s with the same length of the number of all the enities in the paragraph. \n",
    "  # With gather operation we will create a new dynamic sequence axes.\n",
    "  # E.g. \n",
    "  # The entities in order in the paragraph is  \n",
    "  #  @entity1 @entity1 @entity3\n",
    "  # The output of the operation will be\n",
    "  # 1 1 1  \n",
    "  entities_all = sequence.gather(entity_condition, \n",
    "                                 entity_condition, \n",
    "                                 name='entities_all')\n",
    "\n",
    "  # The model prediction is a sequence of probabilities of all the tokens in the paragraph, \n",
    "  # but we only pick answer from the entities. \n",
    "  # With gather operation, we will filter out the probabilities of none entities.\n",
    "  # E.g.\n",
    "  # The model prediction of the above example would be something like\n",
    "  #  0.1 0.2 0.1 0.3 0.2 0.1\n",
    "  # The probabilities of the entities to be the answer will be\n",
    "  #  0.1 0.2 0.1\n",
    "  # With scatter operation, we assign the dynamic axes of entities_all to answers.\n",
    "  answers = sequence.scatter(\n",
    "                             # Only get the predicted probilities of the entities in the paragraph\n",
    "                             sequence.gather(model.outputs[-1], entity_condition), \n",
    "                             entities_all, \n",
    "                             name='Final_Ans')\n",
    "\n",
    "  # entity_ids is the ids of the entities in the paragraph in order. \n",
    "  # It's a sequence of one hot encoded vector. \n",
    "  # The dimention is the maxium number of unique entities in all the paragraphs.\n",
    "  # E.g. \n",
    "  # The ids for the above example will be\n",
    "  #  1:1 1:1 3:1  \n",
    "  entity_ids = input(shape=(params.entity_dim), \n",
    "                     is_sparse=True,   \n",
    "                     # The sequence length is the same as the number of entities in the paragraph, \n",
    "                     #   so it has the same dynamic axes as entities_all, as well as answers.\n",
    "                     dynamic_axes=entities_all.dynamic_axes,\n",
    "                     name='entity_ids')\n",
    "    \n",
    "  # The global token id zero is used for unknown tokens, and entity ids start with 1. \n",
    "  # So we will trim the first column in the entity id matrix.\n",
    "  # E.g. the output for the above example will be\n",
    "  # [1 0 0] [1 0 0] [0 0 1]\n",
    "  entity_id_matrix = slice(\n",
    "                           # entity_ids is one hot encoded sparse vectors, \n",
    "                           # by reshaping it, we convert them to dense vectors. \n",
    "                           reshape(entity_ids, params.entity_dim),\n",
    "                           # It's the last axis\n",
    "                           axis = -1, \n",
    "                           begin_index = 1, \n",
    "                           end_index = params.entity_dim)\n",
    "\n",
    "  # Now by multiplying answers with entity_id_matrix, we will get a probability matrix like\n",
    "  # [0.1 0 0] [0.2 0 0] [0 0 0.1]\n",
    "  entity_probs = element_times(answers, entity_id_matrix)\n",
    "    \n",
    "  # By reducing sum over the sequence dynamic axis, \n",
    "  # we will aggregate the probabilities of the same entities that \n",
    "  #  present at different positions in the paragraph. \n",
    "  # Then we get the probabilities of unique entities in the paragraph.\n",
    "  # E.g. the output for the above example input will be,\n",
    "  # [0.3 0 0.1]\n",
    "  agg_pred = sequence.reduce_sum(entity_probs)\n",
    "  \n",
    "  # We pick the entities with maxium probability as the answer\n",
    "  # E.g. the output for the above example will be,\n",
    "  # [1 0 0]\n",
    "  pred_max = hardmax(agg_pred, name='pred_max')\n",
    "  return pred_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Mapping the prediction to entities\n",
    "The prediction result is a one hot vector that 1 means the entity at that position is the predicted answer and 0 means not. To make the predition result readable, we can convert that vector to entity id and remapping it back to the real entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-dedc262bcb15>, line 66)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-dedc262bcb15>\"\u001b[0;36m, line \u001b[0;32m66\u001b[0m\n\u001b[0;31m    http://10.196.46.172:8888/notebooks/Tutorials/CNTK_302b_ReasoNet_Evaluation_With_CNN_Dataset.ipynb#  for record in content:\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import cntk.device as device\n",
    "import numpy as np\n",
    "import math\n",
    "from cntk import load_model\n",
    "import time\n",
    "def unroll_entities(doc, entity_dict):\n",
    "  tokens = doc.split(u' ')\n",
    "  for i in range(len(tokens)):\n",
    "    if tokens[i] in entity_dict:\n",
    "      tokens[i] = entity_dict[tokens[i]]    \n",
    "  return u' '.join(tokens)  \n",
    "\n",
    "def pred_cnn_model(model_path):\n",
    "  logger.init(\"cnn_test\")\n",
    "  vocab_dim = 101585\n",
    "  entity_dim = 586\n",
    "  hidden_dim=256\n",
    "  max_rl_steps=5\n",
    "  embedding_dim=300\n",
    "  att_dim = 384\n",
    "  minibatch_size=1\n",
    "  share_rnn = True\n",
    "\n",
    "  test_data = create_reader(test_ctf, vocab_dim, entity_dim, False)\n",
    "  embedding_init = None\n",
    "\n",
    "  params = model_params(vocab_dim = vocab_dim, entity_dim = entity_dim, hidden_dim = hidden_dim,\n",
    "                        embedding_dim = embedding_dim, attention_dim=att_dim, max_rl_steps = max_rl_steps,\n",
    "                        embedding_init = embedding_init, dropout_rate = 0.2, share_rnn_param = share_rnn)\n",
    "\n",
    "  entity_table, word_table = Vocabulary.load_vocab(vocab_path)\n",
    "  model = load_model(model_path)\n",
    "  predict_func = predict(model, params)\n",
    "  bind = bind_data(predict_func, test_data)\n",
    "  context_stream = get_context_bind_stream(bind)\n",
    "  samples_sum = 0\n",
    "  i = 0\n",
    "  predicted_results = []\n",
    "  max_num = 5\n",
    "  start = time.time()\n",
    "  while i<test_size:\n",
    "    mbs = min(test_size - i, minibatch_size)\n",
    "    mb = test_data.next_minibatch(mbs, bind)\n",
    "    pred = predict_func.eval(mb)\n",
    "    # Convert entity one hot vector to entity id\n",
    "    ans = np.nonzero(pred)\n",
    "    # Remapping entity id to real entity\n",
    "    for id in ans[1]:\n",
    "      predicted_results += [ entity_table.lookup_by_id(id) ]    \n",
    "    i += mb[context_stream].num_samples\n",
    "    samples = mb[context_stream].num_sequences\n",
    "    samples_sum += samples\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "    if samples_sum >= max_num:\n",
    "      break\n",
    "  end = time.time()\n",
    "  total = end - start\n",
    "  print(\"\")\n",
    "  print(\"Evaluated samples: {0} in {1} seconds\".format(samples_sum, total))\n",
    "  instance_id = 0\n",
    "  with open(raw_test_data, 'r', encoding='utf-8') as raw:\n",
    "    content = raw.readlines()\n",
    "    for record in content:\n",
    "      fields = record.strip().split(u'\\t')\n",
    "      query = fields[0]\n",
    "      answer = fields[1]\n",
    "      doc = fields[2]\n",
    "      entity_dict={}\n",
    "      for i in range(3,len(fields)):\n",
    "        pair=fields[i].split(u':')\n",
    "        entity_dict[pair[0]]=pair[1]\n",
    "      print(\"===============\")\n",
    "      print(\"[{0}] Doc: {1}\\n Query: {2}\\n Answer: {3}\\n Expected: {4}\".format(instance_id, \n",
    "                                                                               doc, \n",
    "                                                                               query, \n",
    "                                                                               predicted_results[instance_id], \n",
    "                                                                               answer))\n",
    "      print(\"=>Unrolled=>\\n Query: {0}\\n Answer: {1}\".format(unroll_entities(query, entity_dict), \n",
    "                                                     unroll_entities(answer, entity_dict)))\n",
    "      print()\n",
    "      instance_id+=1\n",
    "      if instance_id >= len(predicted_results):\n",
    "        break\n",
    "\n",
    "pred_cnn_model(model_path)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
